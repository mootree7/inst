# inst

You're a logical developer who cares about breaking down a problem and taking its solutions and optimizing it in a way that takes into consideration both logic and knowledge of the overhead and time cost of the varying actions in a software development project. I need your help to optimize my solution to a problem, i want you to critique my solution and tell me if it's the best considering software dev methodology and its consequences and also logic. So here's the context: my team is working on a fastapi that's supposed to receive a json object containing an array of objects, each object has the metadata of a specific file (like creation date, last modified date, name, size, etc..) and also a dd_id, which is the id representing the vendor that the files belong to, all files in a single payload will have the same dd_id. The object also has a download_url which can be used to download the actual file from salesforce. The goal of POSTing to the API is to upload the files to a GCP bucket under the vendor id. As in, someone posting a payload of documents pertaining to a specific vendor is expecting that the GCP bucket under that vendor id will contain only the documents in the payload, no more and no less, such that if there were documents beforehand, they would simply be overwritten and deleted. If someone posts one file under a specific vendor for example and the GCP bucket for that vendor contains 9 files from a previous upload and one file out of them is a duplicate of the file being posted, that person is expecting the GCP bucket afterwards to only have that one file, regardless of whether its the file that was inserted or its duplicate that was already there. We were given a heuristic by the people that will be using the api that the majority of documents in a given payload will be duplicates, as in if they post documents once, then they post another batch, that second batch is likely to contain a lot of documents that were posted in the first one. So my thinking is that having deduplication checking based on file metadata before downloading the files and uploading them to gcp would be more efficient. A requirement we have is to keep written record of all documents across all past payloads regardless of whether they were ingested into gcp or not.  My plan which i want you to critique is to do the following:
1- after receiving the post message containing the object, insert the document metadata as records for each document in a sql database, with a field called "status" set by default to "awaiting deduplication". 
2- Then look for documents that are ingested in gcp but not found in the inserted payload in order to mark them to be deleted. This will be done by querying the sql for records that share the same dd_id as the files that just got inserted and that have the status "ingested". The query will look for the ones that don't fulfill equality on all of those fields :(name, file_type, created_date, last_modified_date) with at least one of the records that share the same dd_id and the status of "awaiting deduplication". These records will have their status flipped to "marked for removal".
3- Then loop through records with the status "awaiting deduplication". For every record, check if there exists another record with the same dd_id and the status "ingested" that fulfills equality on all those fields: (name, file_type, created_date, last_modified_date), if there exists another record, then change the status to "not ingested. Is a duplicate". If there doesn't exist another record change the status to "awaiting ingestion"
4- for all the files marked "awaiting ingestion", download them then upload to gcp then change the status to "ingested"
5- for all the files marked "marked for removal", remove them from gcp and change status to "removed from gcp"
